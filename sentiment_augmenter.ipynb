{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# model is from huggingface - https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment-latest\n",
    "model_path = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "sentiment_task = pipeline(\"sentiment-analysis\", model=model_path, tokenizer=model_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary Testing\n",
    "\n",
    "In the following cell, I played around with the model to assess how it can be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hey': [('neutral', 0.5666436), ('positive', 0.37361282), ('negative', 0.05974364)], 'lefty': [('neutral', 0.6846445), ('positive', 0.18011715), ('negative', 0.13523835)], 'loser': [('negative', 0.6023872), ('neutral', 0.287571), ('positive', 0.110041775)], 'how': [('neutral', 0.6041866), ('positive', 0.21852258), ('negative', 0.17729077)], 'about': [('neutral', 0.57544786), ('positive', 0.29396135), ('negative', 0.13059081)], 'they': [('neutral', 0.5719359), ('positive', 0.29150087), ('negative', 0.13656318)], 'take': [('neutral', 0.6289807), ('positive', 0.2486109), ('negative', 0.12240843)], 'commercial': [('neutral', 0.5754338), ('positive', 0.3216852), ('negative', 0.10288105)], 'together': [('neutral', 0.560652), ('positive', 0.34983358), ('negative', 0.08951441)], 'and': [('neutral', 0.57114667), ('positive', 0.27668178), ('negative', 0.15217155)], 'save': [('neutral', 0.6034487), ('positive', 0.2806874), ('negative', 0.11586387)], 'a': [('neutral', 0.4693253), ('positive', 0.38239115), ('negative', 0.14828351)], 'bit!.': [('neutral', 0.6147976), ('positive', 0.28225353), ('negative', 0.10294887)]}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer, AutoConfig\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "\n",
    "MODEL = f\"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "config = AutoConfig.from_pretrained(MODEL)\n",
    "# PT\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL)\n",
    "#model.save_pretrained(MODEL)\n",
    "text = \"hey lefty loser how about they take commercial together and save a bit!.\"\n",
    "query = text.split(\" \")\n",
    "\n",
    "sentiment_output = {}\n",
    "# evaluate sentiments for each word in the query\n",
    "for word in query:\n",
    "    encoded_input = tokenizer(word, return_tensors='pt') # includes both the input_ids and the attention_mask\n",
    "\n",
    "    #print(encoded_input)\n",
    "\n",
    "    output = model(**encoded_input)\n",
    "    scores = output[0][0].detach().numpy()\n",
    "    scores = softmax(scores)\n",
    "\n",
    "    ranking = np.argsort(scores)\n",
    "    ranking = ranking[::-1] # 0 stands for negative, 1 stands for neutral, 2 stands for positive\n",
    "\n",
    "    temp = []\n",
    "    for i in range(scores.shape[0]):\n",
    "        l = config.id2label[ranking[i]]\n",
    "        s = scores[ranking[i]]\n",
    "        temp.append((l, s))\n",
    "    \n",
    "    sentiment_output[word] = temp\n",
    "\n",
    "print(sentiment_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augmentation Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hey [neutral] lefty [neutral] loser [negative] how [neutral] about [neutral] they [neutral] take [neutral] commercial [neutral] together [neutral] and [neutral] save [neutral] a [neutral] bit!. [neutral] \n"
     ]
    }
   ],
   "source": [
    "# Augmentation Example\n",
    "\n",
    "augmented_input = \"\"\n",
    "\n",
    "for word in query:\n",
    "    augmented_input += word + \" \" + f\"[{sentiment_output[word][0][0]}]\" + \" \"\n",
    "\n",
    "print(augmented_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vibe Testing on Test Data\n",
    "\n",
    "In the following example. I show that sentiment analysis by itself is not sufficient in classifying the toxicity of a query. \n",
    "\n",
    "Query: \"I get the odd feeling Klastri  the head of the ACLU of Hawaii  will step in and defend this scum for freedom of speech.\"\n",
    "\n",
    "Most Likely Sentiment: \n",
    "1) neutral\n",
    "2) positive\n",
    "3) negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I get the odd feeling Klastri  the head of the ACLU of Hawaii  will step in and defend this scum for freedom of speech.\n",
      "negative: 0.7978891134262085\n",
      "neutral: 0.19384410977363586\n",
      "positive: 0.008266775868833065\n"
     ]
    }
   ],
   "source": [
    "# Testing on actual test data to see vibe:\n",
    "\n",
    "query = \"I get the odd feeling Klastri  the head of the ACLU of Hawaii  will step in and defend this scum for freedom of speech.\"\n",
    "\n",
    "encoded_input = tokenizer(query, return_tensors='pt') # includes both the input_ids and the attention_mask\n",
    "output = model(**encoded_input)\n",
    "scores = output[0][0].detach().numpy()\n",
    "scores = softmax(scores)\n",
    "\n",
    "ranking = np.argsort(scores)\n",
    "ranking = ranking[::-1] # 0 stands for negative, 1 stands for neutral, 2 stands for positive\n",
    "\n",
    "print(query) # for visualization\n",
    "for i in range(scores.shape[0]):\n",
    "    l = config.id2label[ranking[i]]\n",
    "    s = scores[ranking[i]]\n",
    "    print(f\"{l}: {s}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importance of segmenting sentences\n",
    "\n",
    "In our experiment below, we can see that we are better in isolating toxic parts of a sentences when whe chunk the query.\n",
    "\n",
    "In the same query: \"I get the odd feeling Klastri  the head of the ACLU of Hawaii  will step in and defend this scum for freedom of speech.\"\n",
    "\n",
    "We observe the following:\n",
    "- I get the odd feeling Klastri [neutral]\n",
    "- the head of the ACLU [neutral]\n",
    "- of Hawaii  will step in [neutral]\n",
    "- and defend this scum for freedom [negative]\n",
    "- of speech. [neutral]\n",
    "\n",
    "This observation is crucial since we can takeaway 2 important lessons:\n",
    "1) Segmentation of sentences to smaller pieces can help focus the content (and allow concurrent processing to speed up classification if needed)\n",
    "2) It might be beneficial to only include [negative] or [positive] sentiments and ignore neutral sentiments to avoid confusion. We can observe that neutral is the highest score which dilutes the other sentiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I get the odd feeling Klastri', ' the head of the ACLU', 'of Hawaii  will step in', 'and defend this scum for freedom', 'of speech.']\n",
      "\n",
      "I get the odd feeling Klastri [neutral]\n",
      " the head of the ACLU [neutral]\n",
      "of Hawaii  will step in [neutral]\n",
      "and defend this scum for freedom [negative]\n",
      "of speech. [neutral]\n",
      "\n",
      "I get the odd feeling Klastri [neutral]  the head of the ACLU [neutral] of Hawaii  will step in [neutral] and defend this scum for freedom [negative] of speech. [neutral] \n"
     ]
    }
   ],
   "source": [
    "query = \"I get the odd feeling Klastri  the head of the ACLU of Hawaii  will step in and defend this scum for freedom of speech.\".split(\" \") # turn this into a list\n",
    "\n",
    "def listostring(s):\n",
    "    str1 = \" \"\n",
    "    return (str1.join(s))\n",
    "\n",
    "\n",
    "divisor = 0 \n",
    "split_index = len(query) // 4\n",
    "\n",
    "# divide the query into 4 parts\n",
    "divided_query = []\n",
    "while divisor < len(query):\n",
    "    divided_query.append(query[divisor:divisor+split_index])\n",
    "    divisor += split_index\n",
    "\n",
    "divided_query = [listostring(element) for element in divided_query] # convert the list of lists into a list of strings\n",
    "print(divided_query)\n",
    "print()\n",
    "visual_augmented_input = \"\"\n",
    "augmented_input = \"\"\n",
    "\n",
    "for phrase in divided_query:\n",
    "    encoded_input = tokenizer(phrase, return_tensors='pt') # includes both the input_ids and the attention_mask\n",
    "    output = model(**encoded_input)\n",
    "    scores = output[0][0].detach().numpy()\n",
    "    scores = softmax(scores)\n",
    "\n",
    "    ranking = np.argsort(scores)\n",
    "    ranking = ranking[::-1] # 0 stands for negative, 1 stands for neutral, 2 stands for positive\n",
    "\n",
    "    visual_augmented_input += phrase + \" \" + f\"[{config.id2label[ranking[0]]}]\" + \"\\n\" # for visualization\n",
    "    augmented_input += phrase + \" \" + f\"[{config.id2label[ranking[0]]}]\" + \" \" \n",
    "\n",
    "print(visual_augmented_input)\n",
    "print(augmented_input)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
