{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ryan/miniconda3/envs/snlp/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer, AutoConfig\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.special import softmax\n",
    "from multiprocessing import Pool, cpu_count # for parallel processing\n",
    "import torch\n",
    "\n",
    "MODEL = f\"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "config = AutoConfig.from_pretrained(MODEL)\n",
    "# PT\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL)\n",
    "\n",
    "device = torch.device(\"mps\") \n",
    "model.to(device) # move to gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions\n",
    "# def process_chunk(texts):\n",
    "#     \"\"\"Process a chunk of texts and return augmented texts.\"\"\"\n",
    "#     augmented_texts = [augment_query(text, model, tokenizer, config) for text in texts]\n",
    "#     return augmented_texts\n",
    "\n",
    "# def parallel_process(df, function, n_cores=cpu_count()):\n",
    "#     \"\"\"Split DataFrame into chunks and process each chunk in parallel.\"\"\"\n",
    "#     df_split = np.array_split(df, n_cores)\n",
    "#     pool = Pool(n_cores)\n",
    "#     df = pd.concat(pool.map(function, [chunk[\"text\"].tolist() for chunk in df_split]))\n",
    "#     pool.close()\n",
    "#     pool.join()\n",
    "#     return df\n",
    "\n",
    "def listostring(s):\n",
    "    \"\"\"Converts a list of strings into a single string\n",
    "\n",
    "    Args:\n",
    "        s (List): A list of strings\n",
    "    \"\"\"\n",
    "    str1 = \" \"\n",
    "    output = str1.join(s)\n",
    "    return(output)\n",
    "\n",
    "def augment_query(query, model, tokenizer, config, split_parts=4, device=\"cpu\"):\n",
    "\n",
    "    query = query.split(\" \") # separate into words\n",
    "    # print(query)\n",
    "    divisor = 0 \n",
    "    split_index = len(query) // split_parts if len(query) // split_parts > 0 else 1  # Ensure no zero division\n",
    "\n",
    "    # divide the query into split_parts\n",
    "    divided_query = []\n",
    "    while divisor < len(query):\n",
    "        divided_query.append(query[divisor:divisor+split_index])\n",
    "        divisor += split_index\n",
    "\n",
    "    divided_query = [listostring(element) for element in divided_query] # convert the list of lists into a list of strings\n",
    "\n",
    "    visual_augmented_input = \"\"\n",
    "    augmented_input = \"\"\n",
    "\n",
    "    for phrase in divided_query:\n",
    "        encoded_input = tokenizer(phrase, return_tensors='pt').to(device) # includes both the input_ids and the attention_mask\n",
    "        output = model(**encoded_input)\n",
    "        scores = output.logits[0].detach().cpu().numpy()\n",
    "        scores = softmax(scores)\n",
    "\n",
    "        ranking = np.argsort(scores)\n",
    "        ranking = ranking[::-1] # 0 stands for negative, 1 stands for neutral, 2 stands for positive\n",
    "\n",
    "        visual_augmented_input += phrase + \" \" + f\"[{config.id2label[ranking[0]]}]\" + \"\\n\" # for visualization\n",
    "        augmented_input += phrase + \" \" + f\"[{config.id2label[ranking[0]]}]\" + \" \" \n",
    "    \n",
    "    return augmented_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augmenting Train Data\n",
    "\n",
    "Splitting into 4 - 5 parts of the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded from data/train_2024.csv\n",
      "Augmenting data...\n",
      "Data augmentation completed and saved to augmented_data/train_2024.csv\n"
     ]
    }
   ],
   "source": [
    "original_path = \"data/train_2024.csv\"\n",
    "augmented_path = \"augmented_data/train_2024.csv\"\n",
    "\n",
    "# Load the original CSV file\n",
    "df = pd.read_csv(original_path, quoting=3) # test for the first 1000 first\n",
    "print(f\"Data loaded from {original_path}\")\n",
    "# print(df)\n",
    "\n",
    "# Apply the augmentation function to the 'text' column\n",
    "print(\"Augmenting data...\")\n",
    "df['text'] = df['text'].apply(lambda x: augment_query(x, model, tokenizer, config, device=device))\n",
    "\n",
    "# Save the augmented data to a new CSV file\n",
    "df.to_csv(augmented_path, index=False)\n",
    "\n",
    "print(f\"Data augmentation completed and saved to {augmented_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded from data/test_2024.csv\n",
      "Augmenting data...\n",
      "Data augmentation completed and saved to augmented_data/test_2024.csv\n"
     ]
    }
   ],
   "source": [
    "original_path = \"data/test_2024.csv\"\n",
    "augmented_path = \"augmented_data/test_2024.csv\"\n",
    "\n",
    "# Load the original CSV file\n",
    "df = pd.read_csv(original_path, quoting=3) # test for the first 1000 first\n",
    "print(f\"Data loaded from {original_path}\")\n",
    "# print(df)\n",
    "\n",
    "# Apply the augmentation function to the 'text' column\n",
    "print(\"Augmenting data...\")\n",
    "df['text'] = df['text'].apply(lambda x: augment_query(x, model, tokenizer, config, device=device))\n",
    "\n",
    "# Save the augmented data to a new CSV file\n",
    "df.to_csv(augmented_path, index=False)\n",
    "\n",
    "print(f\"Data augmentation completed and saved to {augmented_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded from data/dev_2024.csv\n",
      "Augmenting data...\n"
     ]
    }
   ],
   "source": [
    "original_path = \"data/dev_2024.csv\"\n",
    "augmented_path = \"augmented_data/dev_2024.csv\"\n",
    "\n",
    "# Load the original CSV file\n",
    "df = pd.read_csv(original_path, quoting=3) # test for the first 1000 first\n",
    "print(f\"Data loaded from {original_path}\")\n",
    "# print(df)\n",
    "\n",
    "# Apply the augmentation function to the 'text' column\n",
    "print(\"Augmenting data...\")\n",
    "df['text'] = df['text'].apply(lambda x: augment_query(x, model, tokenizer, config, device=device))\n",
    "\n",
    "# Save the augmented data to a new CSV file\n",
    "df.to_csv(augmented_path, index=False)\n",
    "\n",
    "print(f\"Data augmentation completed and saved to {augmented_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
