{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c9a4378-8415-4acf-96ba-5fe7e5fd1f3b",
   "metadata": {
    "id": "9c9a4378-8415-4acf-96ba-5fe7e5fd1f3b"
   },
   "source": [
    "### Tutorial From\n",
    "\n",
    "1. https://mccormickml.com/2019/09/19/XLNet-fine-tuning/\n",
    "2. https://medium.com/swlh/using-xlnet-for-sentiment-classification-cfa948e65e85"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jfUtzeAYJaQ4",
   "metadata": {
    "id": "jfUtzeAYJaQ4"
   },
   "source": [
    "### Enabling GPU in Google Colab\n",
    "\n",
    "1. Go to 'Runtime' tab\n",
    "2. Click on 'Change runtime type'\n",
    "3. Select GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50ae906-0bf4-44d8-b434-57725d3ac929",
   "metadata": {
    "id": "d50ae906-0bf4-44d8-b434-57725d3ac929"
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0bwPjAYJC4S3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0bwPjAYJC4S3",
    "outputId": "601ad530-becc-459f-9eb9-6ce69e211d65"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: Keras-Preprocessing in /usr/local/lib/python3.10/dist-packages (1.1.2)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.10/dist-packages (from Keras-Preprocessing) (1.25.2)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from Keras-Preprocessing) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install Keras-Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97dccc61-85c8-4487-b7e9-019c0ee5c821",
   "metadata": {
    "id": "97dccc61-85c8-4487-b7e9-019c0ee5c821"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ryan/miniconda3/envs/snlp/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import TensorDataset,RandomSampler,SequentialSampler,Dataset,DataLoader,IterableDataset\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import transformers\n",
    "from transformers import XLNetTokenizer, XLNetModel, AdamW, XLNetForSequenceClassification\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "O8A7wm5KEz5d",
   "metadata": {
    "id": "O8A7wm5KEz5d"
   },
   "source": [
    "### Use Google Colab GPU if available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "VDlg7P1XFNgE",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VDlg7P1XFNgE",
    "outputId": "e22f7cf0-85c4-4b29-f3e8-6f02bd160842"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS is Available\n"
     ]
    }
   ],
   "source": [
    "# Return a bool indicating if CUDA is currently available.\n",
    "if torch.cuda.is_available():\n",
    "  # Use CUDA-enabled GPU\n",
    "  device = torch.device(\"cuda:0\")\n",
    "  print(\"GPU is Available\")\n",
    "  torch.cuda.empty_cache()\n",
    "  print(torch.cuda.get_device_name(0))\n",
    "  \n",
    "elif torch.backends.mps.is_available(): # added functionality to include mac users\n",
    "  # Use MPS\n",
    "  device = torch.device(\"mps\")\n",
    "  print(\"MPS is Available\")\n",
    "\n",
    "else:\n",
    "  device = torch.device(\"cpu\")\n",
    "  print(\"GPU is Not Available. Use CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4314e7-dc83-4fa3-81e4-1ab5f52ad5c5",
   "metadata": {
    "id": "ab4314e7-dc83-4fa3-81e4-1ab5f52ad5c5"
   },
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "486b20ca-83b6-46cf-907c-e22a00e0756c",
   "metadata": {
    "id": "486b20ca-83b6-46cf-907c-e22a00e0756c"
   },
   "outputs": [],
   "source": [
    "# Read Training Dataset\n",
    "# https://towardsdatascience.com/3-ways-to-load-csv-files-into-colab-7c14fcbdcb92\n",
    "# filepath_train = 'https://raw.githubusercontent.com/johnlohjy/SNLP_Project/XLNet_John/data/train_2024.csv'\n",
    "# from google.colab import files\n",
    "# uploaded = files.upload()\n",
    "filepath_train = 'augmented_data/train_2024_preprocessed.csv'\n",
    "df = pd.read_csv(filepath_train, quoting=3)\n",
    "\n",
    "\n",
    "# Example Code\n",
    "# Upload the train file from your local drive\n",
    "# from google.colab import files\n",
    "# uploaded = files.upload()\n",
    "# df = pd.read_csv(\"in_domain_train.tsv\", delimiter='\\t', header=None, names=['sentence_source', 'label', 'label_notes', 'sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3977f39a-063a-4079-b743-41c58b031f2f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "3977f39a-063a-4079-b743-41c58b031f2f",
    "outputId": "99473013-3296-4b9c-8d42-65460118c912"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Except that Desmond played [neutral] first bas...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>What i find funny is the loyalty and blindness...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Read the article [neutral]  not just [neutral]...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Speaking of a [neutral] horses backside  [neut...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Michael Barone- [neutral] gee are [neutral] yo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                               text  label\n",
       "0   0  Except that Desmond played [neutral] first bas...      0\n",
       "1   1  What i find funny is the loyalty and blindness...      0\n",
       "2   2  Read the article [neutral]  not just [neutral]...      0\n",
       "3   3  Speaking of a [neutral] horses backside  [neut...      1\n",
       "4   4  Michael Barone- [neutral] gee are [neutral] yo...      1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View head of training dataset\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "Sg7NFZPJ1fUG",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Sg7NFZPJ1fUG",
    "outputId": "5b36ae1c-f5b8-49ec-d849-073d74b2eb9b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(99000, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add2aa70-508a-4c21-a3cd-00b4d7f57025",
   "metadata": {
    "id": "add2aa70-508a-4c21-a3cd-00b4d7f57025"
   },
   "source": [
    "### Pre-Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3a7d4d7-0ec2-43fb-8654-b40b48402a6c",
   "metadata": {
    "id": "e3a7d4d7-0ec2-43fb-8654-b40b48402a6c"
   },
   "outputs": [],
   "source": [
    "# Step 1: Get sentences\n",
    "sentences = list(df.loc[:, 'text'])\n",
    "\n",
    "# Example Code\n",
    "# sentences = list(df.loc[:, 'sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d9e3f65d-a2a7-4169-bd6f-4ea4136e48f9",
   "metadata": {
    "id": "d9e3f65d-a2a7-4169-bd6f-4ea4136e48f9"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Step 2: Add Sepcial tokens [SEP] (end of sentence token) and [CLS] (classification token) to the end of sequences first\n",
    "https://datascience.stackexchange.com/questions/66207/what-is-purpose-of-the-cls-token-and-why-is-its-encoding-output-important\n",
    "https://towardsdatascience.com/fastai-with-transformers-bert-roberta-xlnet-xlm-distilbert-4f41ee18ecb2\n",
    "https://huggingface.co/docs/transformers/model_doc/xlnet#xlnettokenizer\n",
    "\"\"\"\n",
    "sentences = [sentence + \" [SEP] [CLS]\" for sentence in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "174c84d1-f9bd-4ded-9285-2e69eb91ef50",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "174c84d1-f9bd-4ded-9285-2e69eb91ef50",
    "outputId": "06cfd187-1da1-418f-bcb8-e7fd088c2001"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Step 3: Initialise tokenizer\n",
    "Initialise word tokenizer to be used\n",
    "SentencePiece Tokenizer is used by XLNetTokenizer. It can handle all words, special characters and spaces easily\n",
    "https://huggingface.co/docs/transformers/en/tokenizer_summary#sentencepiece\n",
    "https://aman.ai/primers/ai/tokenizer/#sentencepiece\n",
    "\"\"\"\n",
    "tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased')\n",
    "\n",
    "# Step 4: Tokenize Text\n",
    "tokenized_text = [tokenizer.tokenize(sent) for sent in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3893d83b-d87f-4c02-a360-901ac97226e4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3893d83b-d87f-4c02-a360-901ac97226e4",
    "outputId": "10dacd12-d2e3-4df1-f0cc-2ac8c19660dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁Except', '▁that', '▁Desmond', '▁played', '▁[', 'neutral', ']', '▁first', '▁base', '▁last', '▁night', '.', '▁[', 'neutral', ']', '▁Tap', 'ia', '▁was', '▁in', '▁', 'LF', '▁[', 'neutral', ']', '▁and', '▁Reynolds', '▁had', '▁[', 'neutral', ']', '▁a', '▁night', '▁off', '.', '▁[', 'neutral', ']', '▁[', 'S', 'EP', ']', '▁[', 'CL', 'S', ']']\n",
      "\n",
      "['▁What', '▁', 'i', '▁find', '▁funny', '▁is', '▁the', '▁loyalty', '▁and', '▁blindness', '▁of', '▁', 'english', '▁community', '.', '▁The', '▁worst', '▁possible', '▁choice', '▁for', '▁them', '▁is', '▁liberal', '▁and', '▁yet', '▁they', '▁keep', '▁voting', '▁for', '▁them', '▁every', '▁time', '.', '▁They', '▁keep', '▁renew', 'ing', '▁hope', '▁every', '▁election', '▁1', '▁year', '▁prior', '▁to', '▁[', 'negative', ']', '▁it', '▁just', '▁to', '▁ignore', '▁them', '▁at', '▁the', '▁winning', '▁', 'sp', 'each', '▁already', '.', '▁', 'Honest', 'ly', '▁P', 'Q', '▁have', '▁more', '▁respect', '▁for', '▁', 'english', '▁community', '▁then', '▁liberal', '▁at', '▁least', '▁they', '▁don', 't', '▁lie', '▁to', '▁you', '▁just', '▁to', '▁get', '▁your', '▁vote', '.', '▁That', '▁being', '▁said', '▁', 'i', '▁don', 't', '▁[', 'negative', ']', '▁vote', '▁P', 'Q', '▁either', '▁tired', '▁of', '▁those', '▁old', '▁man', '▁but', '▁that', '▁is', '▁another', '▁story', '.', '▁I', '▁mostly', '▁vote', '▁local', '▁candidate', '▁regardless', '▁of', '▁party', '▁even', '▁voted', '▁liberal', '▁once', '.', '.', '▁Out', 'ch', '▁that', '▁was', '▁hard', '▁to', '▁admit', '.', '▁But', '▁seriously', '▁guy', \"'\", 's', '▁drop', '▁the', '▁act', '▁anti', '▁P', 'Q', '▁[', 'negative', ']', '▁anti', '▁Q', 'S', '▁don', 't', '▁vote', '▁for', '▁C', 'AQ', '▁cause', '▁they', '▁don', 't', '▁win', '▁etc', '.', '.', '▁Any', '▁of', '▁those', '▁will', '▁at', '▁least', '▁respect', '▁you', '▁when', '▁they', '▁say', '▁no', '.', '▁And', '▁most', '▁of', '▁time', '▁they', '▁will', '▁say', '▁yes', '▁and', '▁', 'ACT', '▁on', '▁it', '▁not', '▁just', '▁saying', '▁it', '▁[', 'negative', ']', '▁like', '▁liberal', 's', '▁do', '.', '▁[', 'neutral', ']', '▁[', 'S', 'EP', ']', '▁[', 'CL', 'S', ']']\n",
      "\n",
      "['▁Read', '▁the', '▁article', '▁[', 'neutral', ']', '▁not', '▁just', '▁[', 'neutral', ']', '▁the', '▁headline', '▁&', '▁[', 'neutral', ']', '▁you', '▁will', '▁find', '▁[', 'positive', ']', '▁out', '.', '▁[', 'neutral', ']', '▁[', 'S', 'EP', ']', '▁[', 'CL', 'S', ']']\n",
      "\n",
      "['▁Speaking', '▁of', '▁a', '▁[', 'neutral', ']', '▁horses', '▁back', 'side', '▁[', 'neutral', ']', '▁is', '▁that', '▁where', '▁[', 'neutral', ']', '▁your', '▁head', '▁is', '▁[', 'neutral', ']', '▁at', '?', '▁[', 'neutral', ']', '▁[', 'S', 'EP', ']', '▁[', 'CL', 'S', ']']\n",
      "\n",
      "['▁Michael', '▁Baron', 'e', '-', '▁[', 'neutral', ']', '▁', 'gee', '▁are', '▁[', 'neutral', ']', '▁you', '▁dumb', '.', '▁[', 'negative', ']', '▁No', '▁[', 'neutral', ']', '▁other', '▁words', '▁[', 'neutral', ']', '▁needed', '.', '▁[', 'neutral', ']', '▁[', 'S', 'EP', ']', '▁[', 'CL', 'S', ']']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Viewing tokenized text\n",
    "for i in range(5):\n",
    "    print(tokenized_text[i])\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b132b3a4-2441-4ce6-b625-45806771dbca",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b132b3a4-2441-4ce6-b625-45806771dbca",
    "outputId": "605e3b4d-d40c-464a-b4fd-6500ed7d5a5d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Max Length of a Sentence is: \n",
      "950\n"
     ]
    }
   ],
   "source": [
    "# Get max length of the sequence\n",
    "MAX_LEN = max(len(sent) for sent in tokenized_text)\n",
    "\n",
    "print(\"The Max Length of a Sentence is: \")\n",
    "print(MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2f36ed61-da38-4070-beba-8344ca914ccd",
   "metadata": {
    "id": "2f36ed61-da38-4070-beba-8344ca914ccd"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Step 5: Prepare inputs for XLNet\n",
    "1) Input IDs\n",
    "   - Seq of integers identifying each input token (from our tokenized text) to its index number in the XLNet tokenizer vocabulary\n",
    "\n",
    "2) Attention Mask\n",
    "   - Helps the model to focus on actual words vs padding\n",
    "\n",
    "3) Labels\n",
    "\"\"\"\n",
    "\n",
    "# Use the XLNet tokenizer to convert the tokens to their index numbers in the XLNet vocabulary\n",
    "input_ids = [tokenizer.convert_tokens_to_ids(token) for token in tokenized_text]\n",
    "\n",
    "# Pad the sequence using keras. Truncate: if len of sequence is less than our MAX_LEN, we cut it from the back\n",
    "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "\n",
    "# Create the attention masks\n",
    "attention_masks = []\n",
    "for sequence in input_ids:\n",
    "    sequence_masked = [float(i>0) for i in sequence]\n",
    "    attention_masks.append(sequence_masked)\n",
    "\n",
    "labels = list(df.loc[:, 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "19cba16f-1d49-4a83-bf2f-85804b467b78",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "19cba16f-1d49-4a83-bf2f-85804b467b78",
    "outputId": "9e1dab86-c3da-4723-cbbd-8b84c4ea5278"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99000\n"
     ]
    }
   ],
   "source": [
    "print(len(input_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a214699a-def9-48e5-bfa3-df17bc4bd283",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a214699a-def9-48e5-bfa3-df17bc4bd283",
    "outputId": "d9aaf703-1149-4af5-b794-7924146900b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99000\n"
     ]
    }
   ],
   "source": [
    "print(len(attention_masks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cb4db30c-0f72-40d7-b10c-5a47df8843ba",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cb4db30c-0f72-40d7-b10c-5a47df8843ba",
    "outputId": "b3484cc5-04d0-4c3e-946d-ef3229e0e00f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99000\n"
     ]
    }
   ],
   "source": [
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddeeb02e-05db-4816-8d2e-2c10df35bf31",
   "metadata": {
    "id": "ddeeb02e-05db-4816-8d2e-2c10df35bf31"
   },
   "source": [
    "### Initialise the PyTorch DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "VPN9L2pE2s83",
   "metadata": {
    "id": "VPN9L2pE2s83"
   },
   "outputs": [],
   "source": [
    "# Define an IterableDataset to tackle memory issue. Stream data.\n",
    "class CustomIterableDataset(IterableDataset):\n",
    "    def __init__(self, input_ids, attention_masks, labels):\n",
    "        self.input_ids = input_ids\n",
    "        self.attention_masks = attention_masks\n",
    "        self.labels = labels\n",
    "\n",
    "    def __iter__(self):\n",
    "        for input_ids, attention_masks, label in zip(self.input_ids, self.attention_masks, self.labels):\n",
    "            # print(label)\n",
    "            # print(\"\")\n",
    "            yield (input_ids, attention_masks, label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5646e222-e482-4468-8cf8-40eed7bde823",
   "metadata": {
    "id": "5646e222-e482-4468-8cf8-40eed7bde823"
   },
   "outputs": [],
   "source": [
    "# Use train_test_split to split our data into train and validation sets for training\n",
    "\n",
    "# Provide the same method for splitting, random state and test size so that inputs and masks match\n",
    "\n",
    "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels,\n",
    "                                                            random_state=2018, test_size=0.1)\n",
    "\n",
    "train_masks, validation_masks, _, _ = train_test_split(attention_masks, input_ids,\n",
    "                                             random_state=2018, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d82e41c2-8604-4729-b3ed-ccd6a7cc2488",
   "metadata": {
    "id": "d82e41c2-8604-4729-b3ed-ccd6a7cc2488"
   },
   "outputs": [],
   "source": [
    "# Convert all of our data into torch tensors, the required datatype for our model\n",
    "\n",
    "train_inputs = torch.tensor(train_inputs)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "\n",
    "validation_inputs = torch.tensor(validation_inputs)\n",
    "validation_labels = torch.tensor(validation_labels)\n",
    "\n",
    "train_masks = torch.tensor(train_masks)\n",
    "validation_masks = torch.tensor(validation_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b0f17002-1e7e-49f6-8c35-a8ce027ea08f",
   "metadata": {
    "id": "b0f17002-1e7e-49f6-8c35-a8ce027ea08f"
   },
   "outputs": [],
   "source": [
    "# Select a batch size for training. For fine-tuning with XLNet, the authors recommend a batch size of\n",
    "# 32, 48, or 128. We will use 32 here to avoid memory issues.\n",
    "batch_size = 4\n",
    "\n",
    "train_dataset = CustomIterableDataset(train_inputs, train_masks, train_labels)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "r66oCLeH3SWo",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r66oCLeH3SWo",
    "outputId": "ec95f813-aafe-4f57-f112-c827b24fa43b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89100\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "auLVO5so3XUP",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "auLVO5so3XUP",
    "outputId": "4a7b9a20-f0ff-4400-c2f0-c569579ddcec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22275\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataloader)) #cuz of batch size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68988495-b07a-4d4d-854a-988305adb7fa",
   "metadata": {
    "id": "68988495-b07a-4d4d-854a-988305adb7fa"
   },
   "source": [
    "### Initialise the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1b551cd8-3fc8-407a-a162-be157858dd92",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1b551cd8-3fc8-407a-a162-be157858dd92",
    "outputId": "5101b97d-6189-4723-ea13-9d17f2ad4fdb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['logits_proj.bias', 'logits_proj.weight', 'sequence_summary.summary.bias', 'sequence_summary.summary.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Load the model: XLNEtForSequenceClassification, the pretrained XLNet model with an added single linear classification layer on top.\n",
    "\n",
    "As we feed input data, the entire pre-trained XLNet model and the additional untrained classification layer is trained on our specific task.\n",
    "\"\"\"\n",
    "model = XLNetForSequenceClassification.from_pretrained(\"xlnet-base-cased\", num_labels=2).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b44ba522-07bf-478a-9534-5fa019c2c410",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b44ba522-07bf-478a-9534-5fa019c2c410",
    "outputId": "06e2e89b-2d47-4ddc-f907-f1474a51c488"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's current parameters:\n"
     ]
    }
   ],
   "source": [
    "# Display model's current parameters\n",
    "print(\"Model's current parameters:\")\n",
    "model_parameters = list(model.named_parameters())\n",
    "# for parameter in model_parameters:\n",
    "    # print(\"Parameter name: \" + parameter[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "486798f2-5c0b-47e9-9108-25d335668e5e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "486798f2-5c0b-47e9-9108-25d335668e5e",
    "outputId": "6d37acbe-d43b-4800-a9a4-fbcc23d4413c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ryan/miniconda3/envs/snlp/lib/python3.10/site-packages/transformers/optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Define a set of different parameters for AdamW to optimise\n",
    "no_decay = ['bias', 'gamma', 'beta']\n",
    "\n",
    "# Which parameters should undergo weight decay and at what rate. Find out more later\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [parameter for name, parameter in model_parameters if not any(no_decay_parameters in name for no_decay_parameters in no_decay)],\n",
    "     'weight_decay_rate': 0.01},\n",
    "\n",
    "    {'params': [parameter for name, parameter in model_parameters if any(no_decay_parameters in name for no_decay_parameters in no_decay)],\n",
    "     'weight_decay_rate': 0.0}\n",
    "]\n",
    "\n",
    "# Initialise the optimzer\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=2e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b51174b-f26a-4465-b2d8-c8a84728a8a7",
   "metadata": {
    "id": "7b51174b-f26a-4465-b2d8-c8a84728a8a7"
   },
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f93b3f9f-43b9-40fc-a3e9-93036756031b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f93b3f9f-43b9-40fc-a3e9-93036756031b",
    "outputId": "784fb6df-9db8-4023-a6e0-595451b9863b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XLNetForSequenceClassification(\n",
      "  (transformer): XLNetModel(\n",
      "    (word_embedding): Embedding(32000, 768)\n",
      "    (layer): ModuleList(\n",
      "      (0-11): 12 x XLNetLayer(\n",
      "        (rel_attn): XLNetRelativeAttention(\n",
      "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ff): XLNetFeedForward(\n",
      "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (activation_function): GELUActivation()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (sequence_summary): SequenceSummary(\n",
      "    (summary): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (activation): Tanh()\n",
      "    (first_dropout): Identity()\n",
      "    (last_dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (logits_proj): Linear(in_features=768, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "aWUkA4il2G38",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aWUkA4il2G38",
    "outputId": "71b57293-0c54-4cde-b47a-09260d1dfc82"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22275\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8d69dbaa-dcc9-4f6d-afbd-17dbc2817978",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8d69dbaa-dcc9-4f6d-afbd-17dbc2817978",
    "outputId": "872db635-c33c-4f19-b7a4-af0162716331"
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 38\u001b[0m\n\u001b[1;32m     33\u001b[0m epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Compute derivatives of loss function wrt parameters\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# When doing backward propagation, PyTorch accumulates the gradients, i.e. the value of computed gradients is added to the grad property of all leaf nodes of computational graph\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Update parameters and take a step using the computed gradient\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# Adjust the parameters by the gradients collected in the backward pass\u001b[39;00m\n\u001b[1;32m     42\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/miniconda3/envs/snlp/lib/python3.10/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/snlp/lib/python3.10/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Number of training epochs (authors recommend between 2 and 4)\n",
    "epochs = 2\n",
    "\n",
    "# trange is a tqdm wrapper around the normal python range\n",
    "for i in range(epochs):\n",
    "  # Set our model to training mode (as opposed to evaluation mode)\n",
    "  model.train()\n",
    "\n",
    "  # Initialise epoch loss\n",
    "  epoch_loss = 0\n",
    "\n",
    "  # Train the data for one epoch\n",
    "  for batch_number, batch in enumerate(train_dataloader):\n",
    "    # Pass data to the specified device as well\n",
    "    batch = tuple(data for data in batch)\n",
    "\n",
    "    # Unpack the inputs from our dataloader\n",
    "    batch_input_ids, batch_input_mask, batch_labels = batch\n",
    "\n",
    "    # Clear out the gradients (by default they accumulate)\n",
    "    # Call optimizer.zero_grad() to reset the gradients of model parameters. Gradients by default add up; to prevent double-counting, we explicitly zero them at each iteration\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward pass\n",
    "    # See https://huggingface.co/docs/transformers/v4.39.1/en/model_doc/xlnet#transformers.XLNetForSequenceClassification\n",
    "    # See https://huggingface.co/docs/transformers/en/model_doc/xlnet#transformers.models.xlnet.modeling_xlnet.XLNetForSequenceClassificationOutput\n",
    "    outputs = model(batch_input_ids.to(device), token_type_ids=None, attention_mask=batch_input_mask.to(device), labels=batch_labels.to(device))\n",
    "    \n",
    "    if not torch.device(\"cpu\"):\n",
    "      batch_input_ids, batch_input_mask, batch_labels = batch_input_ids.to(\"cpu\"), batch_input_mask.to(\"cpu\"), batch_labels.to(\"cpu\") # move all the inputs back to cpu to free up gpu memory\n",
    "    \n",
    "    loss = outputs[0]\n",
    "\n",
    "    epoch_loss += loss.item()\n",
    "\n",
    "    # Backward pass\n",
    "    # Compute derivatives of loss function wrt parameters\n",
    "    # When doing backward propagation, PyTorch accumulates the gradients, i.e. the value of computed gradients is added to the grad property of all leaf nodes of computational graph\n",
    "    loss.backward()\n",
    "\n",
    "    # Update parameters and take a step using the computed gradient\n",
    "    # Adjust the parameters by the gradients collected in the backward pass\n",
    "    optimizer.step()\n",
    "\n",
    "    # del batch_input_ids, batch_input_mask, batch_labels, outputs\n",
    "    \n",
    "\n",
    "    # print(\"\")\n",
    "    # print(\"\")\n",
    "\n",
    "\n",
    "  print(\"Loss for epoch \" + str(i+1) + \" is: \" + str(epoch_loss / len(train_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vVzXL4_OVbB2",
   "metadata": {
    "id": "vVzXL4_OVbB2"
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"xlnetmodel_with_augments.pt\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "V100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
