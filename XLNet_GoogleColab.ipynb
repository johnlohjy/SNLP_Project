{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c9a4378-8415-4acf-96ba-5fe7e5fd1f3b",
   "metadata": {},
   "source": [
    "### Tutorial From\n",
    "\n",
    "1. https://mccormickml.com/2019/09/19/XLNet-fine-tuning/\n",
    "2. https://medium.com/swlh/using-xlnet-for-sentiment-classification-cfa948e65e85"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50ae906-0bf4-44d8-b434-57725d3ac929",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97dccc61-85c8-4487-b7e9-019c0ee5c821",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import XLNetTokenizer, XLNetModel, AdamW, get_linear_schedule_with_warmup\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from collections import defaultdict\n",
    "\n",
    "from torch import nn, optim\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from torch.utils.data import TensorDataset,RandomSampler,SequentialSampler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "652445ad-67d2-4d03-8ac3-6ce225d56246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find out difference between pytorch_transformers vs transformers\n",
    "from transformers import XLNetForSequenceClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4314e7-dc83-4fa3-81e4-1ab5f52ad5c5",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "486b20ca-83b6-46cf-907c-e22a00e0756c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Training Dataset\n",
    "filepath_train = os.path.join(os.getcwd(), 'data', 'train_2024.csv')\n",
    "df = pd.read_csv(filepath_train, quoting=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3977f39a-063a-4079-b743-41c58b031f2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Except that Desmond played first base last nig...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>What i find funny is the loyalty and blindness...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Read the article  not just the headline &amp; you ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Speaking of a horses backside  is that where y...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Michael Barone- gee are you dumb.  No other wo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                               text  label\n",
       "0   0  Except that Desmond played first base last nig...      0\n",
       "1   1  What i find funny is the loyalty and blindness...      0\n",
       "2   2  Read the article  not just the headline & you ...      0\n",
       "3   3  Speaking of a horses backside  is that where y...      1\n",
       "4   4  Michael Barone- gee are you dumb.  No other wo...      1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View head of training dataset\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add2aa70-508a-4c21-a3cd-00b4d7f57025",
   "metadata": {},
   "source": [
    "### Pre-Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e3a7d4d7-0ec2-43fb-8654-b40b48402a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Get sentences\n",
    "sentences = list(df.loc[:, 'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d9e3f65d-a2a7-4169-bd6f-4ea4136e48f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Step 2: Add Sepcial tokens [SEP] (end of sentence token) and [CLS] (classification token) to the end of sequences first\n",
    "https://datascience.stackexchange.com/questions/66207/what-is-purpose-of-the-cls-token-and-why-is-its-encoding-output-important\n",
    "https://towardsdatascience.com/fastai-with-transformers-bert-roberta-xlnet-xlm-distilbert-4f41ee18ecb2\n",
    "https://huggingface.co/docs/transformers/model_doc/xlnet#xlnettokenizer\n",
    "\"\"\"\n",
    "sentences = [sentence + \" [SEP] [CLS]\" for sentence in sentences]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "174c84d1-f9bd-4ded-9285-2e69eb91ef50",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Step 3: Initialise tokenizer\n",
    "Initialise word tokenizer to be used\n",
    "SentencePiece Tokenizer is used by XLNetTokenizer. It can handle all words, special characters and spaces easily\n",
    "https://huggingface.co/docs/transformers/en/tokenizer_summary#sentencepiece\n",
    "https://aman.ai/primers/ai/tokenizer/#sentencepiece\n",
    "\"\"\"\n",
    "tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased') \n",
    "\n",
    "# Step 4: Tokenize Text\n",
    "tokenized_text = [tokenizer.tokenize(sent) for sent in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3893d83b-d87f-4c02-a360-901ac97226e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁Except', '▁that', '▁Desmond', '▁played', '▁first', '▁base', '▁last', '▁night', '.', '▁Tap', 'ia', '▁was', '▁in', '▁', 'LF', '▁and', '▁Reynolds', '▁had', '▁a', '▁night', '▁off', '.', '▁[', 'S', 'EP', ']', '▁[', 'CL', 'S', ']']\n",
      "\n",
      "['▁What', '▁', 'i', '▁find', '▁funny', '▁is', '▁the', '▁loyalty', '▁and', '▁blindness', '▁of', '▁', 'english', '▁community', '.', '▁The', '▁worst', '▁possible', '▁choice', '▁for', '▁them', '▁is', '▁liberal', '▁and', '▁yet', '▁they', '▁keep', '▁voting', '▁for', '▁them', '▁every', '▁time', '.', '▁They', '▁keep', '▁renew', 'ing', '▁hope', '▁every', '▁election', '▁1', '▁year', '▁prior', '▁to', '▁it', '▁just', '▁to', '▁ignore', '▁them', '▁at', '▁the', '▁winning', '▁', 'sp', 'each', '▁already', '.', '▁', 'Honest', 'ly', '▁P', 'Q', '▁have', '▁more', '▁respect', '▁for', '▁', 'english', '▁community', '▁then', '▁liberal', '▁at', '▁least', '▁they', '▁don', 't', '▁lie', '▁to', '▁you', '▁just', '▁to', '▁get', '▁your', '▁vote', '.', '▁That', '▁being', '▁said', '▁', 'i', '▁don', 't', '▁vote', '▁P', 'Q', '▁either', '▁tired', '▁of', '▁those', '▁old', '▁man', '▁but', '▁that', '▁is', '▁another', '▁story', '.', '▁I', '▁mostly', '▁vote', '▁local', '▁candidate', '▁regardless', '▁of', '▁party', '▁even', '▁voted', '▁liberal', '▁once', '.', '.', '▁Out', 'ch', '▁that', '▁was', '▁hard', '▁to', '▁admit', '.', '▁But', '▁seriously', '▁guy', \"'\", 's', '▁drop', '▁the', '▁act', '▁anti', '▁P', 'Q', '▁anti', '▁Q', 'S', '▁don', 't', '▁vote', '▁for', '▁C', 'AQ', '▁cause', '▁they', '▁don', 't', '▁win', '▁etc', '.', '.', '▁Any', '▁of', '▁those', '▁will', '▁at', '▁least', '▁respect', '▁you', '▁when', '▁they', '▁say', '▁no', '.', '▁And', '▁most', '▁of', '▁time', '▁they', '▁will', '▁say', '▁yes', '▁and', '▁', 'ACT', '▁on', '▁it', '▁not', '▁just', '▁saying', '▁it', '▁like', '▁liberal', 's', '▁do', '.', '▁[', 'S', 'EP', ']', '▁[', 'CL', 'S', ']']\n",
      "\n",
      "['▁Read', '▁the', '▁article', '▁not', '▁just', '▁the', '▁headline', '▁&', '▁you', '▁will', '▁find', '▁out', '.', '▁[', 'S', 'EP', ']', '▁[', 'CL', 'S', ']']\n",
      "\n",
      "['▁Speaking', '▁of', '▁a', '▁horses', '▁back', 'side', '▁is', '▁that', '▁where', '▁your', '▁head', '▁is', '▁at', '?', '▁[', 'S', 'EP', ']', '▁[', 'CL', 'S', ']']\n",
      "\n",
      "['▁Michael', '▁Baron', 'e', '-', '▁', 'gee', '▁are', '▁you', '▁dumb', '.', '▁No', '▁other', '▁words', '▁needed', '.', '▁[', 'S', 'EP', ']', '▁[', 'CL', 'S', ']']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Viewing tokenized text\n",
    "for i in range(5):\n",
    "    print(tokenized_text[i])\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b132b3a4-2441-4ce6-b625-45806771dbca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Max Length of a Sentence is: \n",
      "935\n"
     ]
    }
   ],
   "source": [
    "# Get max length of the sequence\n",
    "MAX_LEN = max(len(sent) for sent in tokenized_text)\n",
    "\n",
    "print(\"The Max Length of a Sentence is: \")\n",
    "print(MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2f36ed61-da38-4070-beba-8344ca914ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Step 5: Prepare inputs for XLNet\n",
    "1) Input IDs\n",
    "   - Seq of integers identifying each input token (from our tokenized text) to its index number in the XLNet tokenizer vocabulary\n",
    "\n",
    "2) Attention Mask\n",
    "   - Helps the model to focus on actual words vs padding\n",
    "\n",
    "3) Labels\n",
    "\"\"\"\n",
    "\n",
    "# Use the XLNet tokenizer to convert the tokens to their index numbers in the XLNet vocabulary\n",
    "input_ids = [tokenizer.convert_tokens_to_ids(token) for token in tokenized_text]\n",
    "\n",
    "# Pad the sequence using keras. Truncate: if len of sequence is less than our MAX_LEN, we cut it from the back\n",
    "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "\n",
    "# Create the attention masks\n",
    "attention_masks = []\n",
    "for sequence in input_ids:\n",
    "    sequence_masked = [float(i>0) for i in sequence]\n",
    "    attention_masks.append(sequence_masked)\n",
    "\n",
    "labels = list(df.loc[:, 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "19cba16f-1d49-4a83-bf2f-85804b467b78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99000\n"
     ]
    }
   ],
   "source": [
    "print(len(input_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a214699a-def9-48e5-bfa3-df17bc4bd283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99000\n"
     ]
    }
   ],
   "source": [
    "print(len(attention_masks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cb4db30c-0f72-40d7-b10c-5a47df8843ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99000\n"
     ]
    }
   ],
   "source": [
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddeeb02e-05db-4816-8d2e-2c10df35bf31",
   "metadata": {},
   "source": [
    "### Initialise the PyTorch DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5646e222-e482-4468-8cf8-40eed7bde823",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use train_test_split to split our data into train and validation sets for training\n",
    "\n",
    "# Provide the same method for splitting, random state and test size so that inputs and masks match\n",
    "\n",
    "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, \n",
    "                                                            random_state=2018, test_size=0.1)\n",
    "\n",
    "train_masks, validation_masks, _, _ = train_test_split(attention_masks, input_ids,\n",
    "                                             random_state=2018, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d82e41c2-8604-4729-b3ed-ccd6a7cc2488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all of our data into torch tensors, the required datatype for our model\n",
    "\n",
    "train_inputs = torch.tensor(train_inputs)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "\n",
    "validation_inputs = torch.tensor(validation_inputs)\n",
    "validation_labels = torch.tensor(validation_labels)\n",
    "\n",
    "train_masks = torch.tensor(train_masks)\n",
    "validation_masks = torch.tensor(validation_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b0f17002-1e7e-49f6-8c35-a8ce027ea08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a batch size for training. For fine-tuning with XLNet, the authors recommend a batch size of \n",
    "# 32, 48, or 128. We will use 32 here to avoid memory issues.\n",
    "batch_size = 32\n",
    "\n",
    "# Create an iterator of our data with torch DataLoader. This helps save on memory during training because, unlike a for loop, \n",
    "# with an iterator the entire dataset does not need to be loaded into memory\n",
    "\n",
    "# Use TensorDataset to make a tuple of sample (train_inputs, train_masks, train_labels)\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "# Shuffle the training samples by randomly sampling the data\n",
    "train_sampler = RandomSampler(train_data)\n",
    "# DataLoader wraps an iterable around to enable easy access to the samples\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "# Sample in a fixed, sequential order\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68988495-b07a-4d4d-854a-988305adb7fa",
   "metadata": {},
   "source": [
    "### Initialise the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1b551cd8-3fc8-407a-a162-be157858dd92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3f74e40bef74f188d2aa248b3acc15c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/467M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['logits_proj.bias', 'logits_proj.weight', 'sequence_summary.summary.bias', 'sequence_summary.summary.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Load the model: XLNEtForSequenceClassification, the pretrained XLNet model with an added single linear classification layer on top.\n",
    "\n",
    "As we feed input data, the entire pre-trained XLNet model and the additional untrained classification layer is trained on our specific task.\n",
    "\"\"\"\n",
    "model = XLNetForSequenceClassification.from_pretrained(\"xlnet-base-cased\", num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b44ba522-07bf-478a-9534-5fa019c2c410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's current parameters:\n",
      "Parameter name: transformer.mask_emb\n",
      "Parameter name: transformer.word_embedding.weight\n",
      "Parameter name: transformer.layer.0.rel_attn.q\n",
      "Parameter name: transformer.layer.0.rel_attn.k\n",
      "Parameter name: transformer.layer.0.rel_attn.v\n",
      "Parameter name: transformer.layer.0.rel_attn.o\n",
      "Parameter name: transformer.layer.0.rel_attn.r\n",
      "Parameter name: transformer.layer.0.rel_attn.r_r_bias\n",
      "Parameter name: transformer.layer.0.rel_attn.r_s_bias\n",
      "Parameter name: transformer.layer.0.rel_attn.r_w_bias\n",
      "Parameter name: transformer.layer.0.rel_attn.seg_embed\n",
      "Parameter name: transformer.layer.0.rel_attn.layer_norm.weight\n",
      "Parameter name: transformer.layer.0.rel_attn.layer_norm.bias\n",
      "Parameter name: transformer.layer.0.ff.layer_norm.weight\n",
      "Parameter name: transformer.layer.0.ff.layer_norm.bias\n",
      "Parameter name: transformer.layer.0.ff.layer_1.weight\n",
      "Parameter name: transformer.layer.0.ff.layer_1.bias\n",
      "Parameter name: transformer.layer.0.ff.layer_2.weight\n",
      "Parameter name: transformer.layer.0.ff.layer_2.bias\n",
      "Parameter name: transformer.layer.1.rel_attn.q\n",
      "Parameter name: transformer.layer.1.rel_attn.k\n",
      "Parameter name: transformer.layer.1.rel_attn.v\n",
      "Parameter name: transformer.layer.1.rel_attn.o\n",
      "Parameter name: transformer.layer.1.rel_attn.r\n",
      "Parameter name: transformer.layer.1.rel_attn.r_r_bias\n",
      "Parameter name: transformer.layer.1.rel_attn.r_s_bias\n",
      "Parameter name: transformer.layer.1.rel_attn.r_w_bias\n",
      "Parameter name: transformer.layer.1.rel_attn.seg_embed\n",
      "Parameter name: transformer.layer.1.rel_attn.layer_norm.weight\n",
      "Parameter name: transformer.layer.1.rel_attn.layer_norm.bias\n",
      "Parameter name: transformer.layer.1.ff.layer_norm.weight\n",
      "Parameter name: transformer.layer.1.ff.layer_norm.bias\n",
      "Parameter name: transformer.layer.1.ff.layer_1.weight\n",
      "Parameter name: transformer.layer.1.ff.layer_1.bias\n",
      "Parameter name: transformer.layer.1.ff.layer_2.weight\n",
      "Parameter name: transformer.layer.1.ff.layer_2.bias\n",
      "Parameter name: transformer.layer.2.rel_attn.q\n",
      "Parameter name: transformer.layer.2.rel_attn.k\n",
      "Parameter name: transformer.layer.2.rel_attn.v\n",
      "Parameter name: transformer.layer.2.rel_attn.o\n",
      "Parameter name: transformer.layer.2.rel_attn.r\n",
      "Parameter name: transformer.layer.2.rel_attn.r_r_bias\n",
      "Parameter name: transformer.layer.2.rel_attn.r_s_bias\n",
      "Parameter name: transformer.layer.2.rel_attn.r_w_bias\n",
      "Parameter name: transformer.layer.2.rel_attn.seg_embed\n",
      "Parameter name: transformer.layer.2.rel_attn.layer_norm.weight\n",
      "Parameter name: transformer.layer.2.rel_attn.layer_norm.bias\n",
      "Parameter name: transformer.layer.2.ff.layer_norm.weight\n",
      "Parameter name: transformer.layer.2.ff.layer_norm.bias\n",
      "Parameter name: transformer.layer.2.ff.layer_1.weight\n",
      "Parameter name: transformer.layer.2.ff.layer_1.bias\n",
      "Parameter name: transformer.layer.2.ff.layer_2.weight\n",
      "Parameter name: transformer.layer.2.ff.layer_2.bias\n",
      "Parameter name: transformer.layer.3.rel_attn.q\n",
      "Parameter name: transformer.layer.3.rel_attn.k\n",
      "Parameter name: transformer.layer.3.rel_attn.v\n",
      "Parameter name: transformer.layer.3.rel_attn.o\n",
      "Parameter name: transformer.layer.3.rel_attn.r\n",
      "Parameter name: transformer.layer.3.rel_attn.r_r_bias\n",
      "Parameter name: transformer.layer.3.rel_attn.r_s_bias\n",
      "Parameter name: transformer.layer.3.rel_attn.r_w_bias\n",
      "Parameter name: transformer.layer.3.rel_attn.seg_embed\n",
      "Parameter name: transformer.layer.3.rel_attn.layer_norm.weight\n",
      "Parameter name: transformer.layer.3.rel_attn.layer_norm.bias\n",
      "Parameter name: transformer.layer.3.ff.layer_norm.weight\n",
      "Parameter name: transformer.layer.3.ff.layer_norm.bias\n",
      "Parameter name: transformer.layer.3.ff.layer_1.weight\n",
      "Parameter name: transformer.layer.3.ff.layer_1.bias\n",
      "Parameter name: transformer.layer.3.ff.layer_2.weight\n",
      "Parameter name: transformer.layer.3.ff.layer_2.bias\n",
      "Parameter name: transformer.layer.4.rel_attn.q\n",
      "Parameter name: transformer.layer.4.rel_attn.k\n",
      "Parameter name: transformer.layer.4.rel_attn.v\n",
      "Parameter name: transformer.layer.4.rel_attn.o\n",
      "Parameter name: transformer.layer.4.rel_attn.r\n",
      "Parameter name: transformer.layer.4.rel_attn.r_r_bias\n",
      "Parameter name: transformer.layer.4.rel_attn.r_s_bias\n",
      "Parameter name: transformer.layer.4.rel_attn.r_w_bias\n",
      "Parameter name: transformer.layer.4.rel_attn.seg_embed\n",
      "Parameter name: transformer.layer.4.rel_attn.layer_norm.weight\n",
      "Parameter name: transformer.layer.4.rel_attn.layer_norm.bias\n",
      "Parameter name: transformer.layer.4.ff.layer_norm.weight\n",
      "Parameter name: transformer.layer.4.ff.layer_norm.bias\n",
      "Parameter name: transformer.layer.4.ff.layer_1.weight\n",
      "Parameter name: transformer.layer.4.ff.layer_1.bias\n",
      "Parameter name: transformer.layer.4.ff.layer_2.weight\n",
      "Parameter name: transformer.layer.4.ff.layer_2.bias\n",
      "Parameter name: transformer.layer.5.rel_attn.q\n",
      "Parameter name: transformer.layer.5.rel_attn.k\n",
      "Parameter name: transformer.layer.5.rel_attn.v\n",
      "Parameter name: transformer.layer.5.rel_attn.o\n",
      "Parameter name: transformer.layer.5.rel_attn.r\n",
      "Parameter name: transformer.layer.5.rel_attn.r_r_bias\n",
      "Parameter name: transformer.layer.5.rel_attn.r_s_bias\n",
      "Parameter name: transformer.layer.5.rel_attn.r_w_bias\n",
      "Parameter name: transformer.layer.5.rel_attn.seg_embed\n",
      "Parameter name: transformer.layer.5.rel_attn.layer_norm.weight\n",
      "Parameter name: transformer.layer.5.rel_attn.layer_norm.bias\n",
      "Parameter name: transformer.layer.5.ff.layer_norm.weight\n",
      "Parameter name: transformer.layer.5.ff.layer_norm.bias\n",
      "Parameter name: transformer.layer.5.ff.layer_1.weight\n",
      "Parameter name: transformer.layer.5.ff.layer_1.bias\n",
      "Parameter name: transformer.layer.5.ff.layer_2.weight\n",
      "Parameter name: transformer.layer.5.ff.layer_2.bias\n",
      "Parameter name: transformer.layer.6.rel_attn.q\n",
      "Parameter name: transformer.layer.6.rel_attn.k\n",
      "Parameter name: transformer.layer.6.rel_attn.v\n",
      "Parameter name: transformer.layer.6.rel_attn.o\n",
      "Parameter name: transformer.layer.6.rel_attn.r\n",
      "Parameter name: transformer.layer.6.rel_attn.r_r_bias\n",
      "Parameter name: transformer.layer.6.rel_attn.r_s_bias\n",
      "Parameter name: transformer.layer.6.rel_attn.r_w_bias\n",
      "Parameter name: transformer.layer.6.rel_attn.seg_embed\n",
      "Parameter name: transformer.layer.6.rel_attn.layer_norm.weight\n",
      "Parameter name: transformer.layer.6.rel_attn.layer_norm.bias\n",
      "Parameter name: transformer.layer.6.ff.layer_norm.weight\n",
      "Parameter name: transformer.layer.6.ff.layer_norm.bias\n",
      "Parameter name: transformer.layer.6.ff.layer_1.weight\n",
      "Parameter name: transformer.layer.6.ff.layer_1.bias\n",
      "Parameter name: transformer.layer.6.ff.layer_2.weight\n",
      "Parameter name: transformer.layer.6.ff.layer_2.bias\n",
      "Parameter name: transformer.layer.7.rel_attn.q\n",
      "Parameter name: transformer.layer.7.rel_attn.k\n",
      "Parameter name: transformer.layer.7.rel_attn.v\n",
      "Parameter name: transformer.layer.7.rel_attn.o\n",
      "Parameter name: transformer.layer.7.rel_attn.r\n",
      "Parameter name: transformer.layer.7.rel_attn.r_r_bias\n",
      "Parameter name: transformer.layer.7.rel_attn.r_s_bias\n",
      "Parameter name: transformer.layer.7.rel_attn.r_w_bias\n",
      "Parameter name: transformer.layer.7.rel_attn.seg_embed\n",
      "Parameter name: transformer.layer.7.rel_attn.layer_norm.weight\n",
      "Parameter name: transformer.layer.7.rel_attn.layer_norm.bias\n",
      "Parameter name: transformer.layer.7.ff.layer_norm.weight\n",
      "Parameter name: transformer.layer.7.ff.layer_norm.bias\n",
      "Parameter name: transformer.layer.7.ff.layer_1.weight\n",
      "Parameter name: transformer.layer.7.ff.layer_1.bias\n",
      "Parameter name: transformer.layer.7.ff.layer_2.weight\n",
      "Parameter name: transformer.layer.7.ff.layer_2.bias\n",
      "Parameter name: transformer.layer.8.rel_attn.q\n",
      "Parameter name: transformer.layer.8.rel_attn.k\n",
      "Parameter name: transformer.layer.8.rel_attn.v\n",
      "Parameter name: transformer.layer.8.rel_attn.o\n",
      "Parameter name: transformer.layer.8.rel_attn.r\n",
      "Parameter name: transformer.layer.8.rel_attn.r_r_bias\n",
      "Parameter name: transformer.layer.8.rel_attn.r_s_bias\n",
      "Parameter name: transformer.layer.8.rel_attn.r_w_bias\n",
      "Parameter name: transformer.layer.8.rel_attn.seg_embed\n",
      "Parameter name: transformer.layer.8.rel_attn.layer_norm.weight\n",
      "Parameter name: transformer.layer.8.rel_attn.layer_norm.bias\n",
      "Parameter name: transformer.layer.8.ff.layer_norm.weight\n",
      "Parameter name: transformer.layer.8.ff.layer_norm.bias\n",
      "Parameter name: transformer.layer.8.ff.layer_1.weight\n",
      "Parameter name: transformer.layer.8.ff.layer_1.bias\n",
      "Parameter name: transformer.layer.8.ff.layer_2.weight\n",
      "Parameter name: transformer.layer.8.ff.layer_2.bias\n",
      "Parameter name: transformer.layer.9.rel_attn.q\n",
      "Parameter name: transformer.layer.9.rel_attn.k\n",
      "Parameter name: transformer.layer.9.rel_attn.v\n",
      "Parameter name: transformer.layer.9.rel_attn.o\n",
      "Parameter name: transformer.layer.9.rel_attn.r\n",
      "Parameter name: transformer.layer.9.rel_attn.r_r_bias\n",
      "Parameter name: transformer.layer.9.rel_attn.r_s_bias\n",
      "Parameter name: transformer.layer.9.rel_attn.r_w_bias\n",
      "Parameter name: transformer.layer.9.rel_attn.seg_embed\n",
      "Parameter name: transformer.layer.9.rel_attn.layer_norm.weight\n",
      "Parameter name: transformer.layer.9.rel_attn.layer_norm.bias\n",
      "Parameter name: transformer.layer.9.ff.layer_norm.weight\n",
      "Parameter name: transformer.layer.9.ff.layer_norm.bias\n",
      "Parameter name: transformer.layer.9.ff.layer_1.weight\n",
      "Parameter name: transformer.layer.9.ff.layer_1.bias\n",
      "Parameter name: transformer.layer.9.ff.layer_2.weight\n",
      "Parameter name: transformer.layer.9.ff.layer_2.bias\n",
      "Parameter name: transformer.layer.10.rel_attn.q\n",
      "Parameter name: transformer.layer.10.rel_attn.k\n",
      "Parameter name: transformer.layer.10.rel_attn.v\n",
      "Parameter name: transformer.layer.10.rel_attn.o\n",
      "Parameter name: transformer.layer.10.rel_attn.r\n",
      "Parameter name: transformer.layer.10.rel_attn.r_r_bias\n",
      "Parameter name: transformer.layer.10.rel_attn.r_s_bias\n",
      "Parameter name: transformer.layer.10.rel_attn.r_w_bias\n",
      "Parameter name: transformer.layer.10.rel_attn.seg_embed\n",
      "Parameter name: transformer.layer.10.rel_attn.layer_norm.weight\n",
      "Parameter name: transformer.layer.10.rel_attn.layer_norm.bias\n",
      "Parameter name: transformer.layer.10.ff.layer_norm.weight\n",
      "Parameter name: transformer.layer.10.ff.layer_norm.bias\n",
      "Parameter name: transformer.layer.10.ff.layer_1.weight\n",
      "Parameter name: transformer.layer.10.ff.layer_1.bias\n",
      "Parameter name: transformer.layer.10.ff.layer_2.weight\n",
      "Parameter name: transformer.layer.10.ff.layer_2.bias\n",
      "Parameter name: transformer.layer.11.rel_attn.q\n",
      "Parameter name: transformer.layer.11.rel_attn.k\n",
      "Parameter name: transformer.layer.11.rel_attn.v\n",
      "Parameter name: transformer.layer.11.rel_attn.o\n",
      "Parameter name: transformer.layer.11.rel_attn.r\n",
      "Parameter name: transformer.layer.11.rel_attn.r_r_bias\n",
      "Parameter name: transformer.layer.11.rel_attn.r_s_bias\n",
      "Parameter name: transformer.layer.11.rel_attn.r_w_bias\n",
      "Parameter name: transformer.layer.11.rel_attn.seg_embed\n",
      "Parameter name: transformer.layer.11.rel_attn.layer_norm.weight\n",
      "Parameter name: transformer.layer.11.rel_attn.layer_norm.bias\n",
      "Parameter name: transformer.layer.11.ff.layer_norm.weight\n",
      "Parameter name: transformer.layer.11.ff.layer_norm.bias\n",
      "Parameter name: transformer.layer.11.ff.layer_1.weight\n",
      "Parameter name: transformer.layer.11.ff.layer_1.bias\n",
      "Parameter name: transformer.layer.11.ff.layer_2.weight\n",
      "Parameter name: transformer.layer.11.ff.layer_2.bias\n",
      "Parameter name: sequence_summary.summary.weight\n",
      "Parameter name: sequence_summary.summary.bias\n",
      "Parameter name: logits_proj.weight\n",
      "Parameter name: logits_proj.bias\n"
     ]
    }
   ],
   "source": [
    "# Display model's current parameters\n",
    "print(\"Model's current parameters:\")\n",
    "model_parameters = list(model.named_parameters())\n",
    "for parameter in model_parameters:\n",
    "    print(\"Parameter name: \" + parameter[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "486798f2-5c0b-47e9-9108-25d335668e5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\johnl\\miniconda3\\envs\\myenv\\lib\\site-packages\\transformers\\optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Define a set of different parameters for AdamW to optimise\n",
    "no_decay = ['bias', 'gamma', 'beta']\n",
    "\n",
    "# Which parameters should undergo weight decay and at what rate. Find out more later\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [parameter for name, parameter in model_parameters if not any(no_decay_parameters in name for no_decay_parameters in no_decay)],\n",
    "     'weight_decay_rate': 0.01},\n",
    "    \n",
    "    {'params': [parameter for name, parameter in model_parameters if any(no_decay_parameters in name for no_decay_parameters in no_decay)],\n",
    "     'weight_decay_rate': 0.0}\n",
    "]\n",
    "\n",
    "# Initialise the optimzer\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=2e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b51174b-f26a-4465-b2d8-c8a84728a8a7",
   "metadata": {},
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f93b3f9f-43b9-40fc-a3e9-93036756031b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XLNetForSequenceClassification(\n",
      "  (transformer): XLNetModel(\n",
      "    (word_embedding): Embedding(32000, 768)\n",
      "    (layer): ModuleList(\n",
      "      (0-11): 12 x XLNetLayer(\n",
      "        (rel_attn): XLNetRelativeAttention(\n",
      "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ff): XLNetFeedForward(\n",
      "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (activation_function): GELUActivation()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (sequence_summary): SequenceSummary(\n",
      "    (summary): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (activation): Tanh()\n",
      "    (first_dropout): Identity()\n",
      "    (last_dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (logits_proj): Linear(in_features=768, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8d69dbaa-dcc9-4f6d-afbd-17dbc2817978",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[52], line 24\u001b[0m\n\u001b[0;32m     19\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# See https://huggingface.co/docs/transformers/v4.39.1/en/model_doc/xlnet#transformers.XLNetForSequenceClassification\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# See https://huggingface.co/docs/transformers/en/model_doc/xlnet#transformers.models.xlnet.modeling_xlnet.XLNetForSequenceClassificationOutput\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_input_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_input_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_labels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]  \n\u001b[0;32m     27\u001b[0m epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\myenv\\lib\\site-packages\\transformers\\models\\xlnet\\modeling_xlnet.py:1545\u001b[0m, in \u001b[0;36mXLNetForSequenceClassification.forward\u001b[1;34m(self, input_ids, attention_mask, mems, perm_mask, target_mapping, token_type_ids, input_mask, head_mask, inputs_embeds, labels, use_mems, output_attentions, output_hidden_states, return_dict, **kwargs)\u001b[0m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1539\u001b[0m \u001b[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[0;32m   1540\u001b[0m \u001b[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[0;32m   1541\u001b[0m \u001b[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[0;32m   1542\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1543\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1545\u001b[0m transformer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer(\n\u001b[0;32m   1546\u001b[0m     input_ids,\n\u001b[0;32m   1547\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m   1548\u001b[0m     mems\u001b[38;5;241m=\u001b[39mmems,\n\u001b[0;32m   1549\u001b[0m     perm_mask\u001b[38;5;241m=\u001b[39mperm_mask,\n\u001b[0;32m   1550\u001b[0m     target_mapping\u001b[38;5;241m=\u001b[39mtarget_mapping,\n\u001b[0;32m   1551\u001b[0m     token_type_ids\u001b[38;5;241m=\u001b[39mtoken_type_ids,\n\u001b[0;32m   1552\u001b[0m     input_mask\u001b[38;5;241m=\u001b[39minput_mask,\n\u001b[0;32m   1553\u001b[0m     head_mask\u001b[38;5;241m=\u001b[39mhead_mask,\n\u001b[0;32m   1554\u001b[0m     inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[0;32m   1555\u001b[0m     use_mems\u001b[38;5;241m=\u001b[39muse_mems,\n\u001b[0;32m   1556\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m   1557\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[0;32m   1558\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[0;32m   1559\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   1560\u001b[0m )\n\u001b[0;32m   1561\u001b[0m output \u001b[38;5;241m=\u001b[39m transformer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1563\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msequence_summary(output)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\myenv\\lib\\site-packages\\transformers\\models\\xlnet\\modeling_xlnet.py:1237\u001b[0m, in \u001b[0;36mXLNetModel.forward\u001b[1;34m(self, input_ids, attention_mask, mems, perm_mask, target_mapping, token_type_ids, input_mask, head_mask, inputs_embeds, use_mems, output_attentions, output_hidden_states, return_dict, **kwargs)\u001b[0m\n\u001b[0;32m   1234\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states:\n\u001b[0;32m   1235\u001b[0m     hidden_states\u001b[38;5;241m.\u001b[39mappend((output_h, output_g) \u001b[38;5;28;01mif\u001b[39;00m output_g \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m output_h)\n\u001b[1;32m-> 1237\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1238\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_h\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1239\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_g\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1240\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattn_mask_h\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnon_tgt_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1241\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattn_mask_g\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1242\u001b[0m \u001b[43m    \u001b[49m\u001b[43mr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpos_emb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1243\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseg_mat\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseg_mat\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1244\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmems\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmems\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1245\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1246\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1247\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1248\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1249\u001b[0m output_h, output_g \u001b[38;5;241m=\u001b[39m outputs[:\u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m   1250\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\myenv\\lib\\site-packages\\transformers\\models\\xlnet\\modeling_xlnet.py:508\u001b[0m, in \u001b[0;36mXLNetLayer.forward\u001b[1;34m(self, output_h, output_g, attn_mask_h, attn_mask_g, r, seg_mat, mems, target_mapping, head_mask, output_attentions)\u001b[0m\n\u001b[0;32m    495\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    496\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    497\u001b[0m     output_h,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    506\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    507\u001b[0m ):\n\u001b[1;32m--> 508\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrel_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    509\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_h\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    510\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_g\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    511\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattn_mask_h\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    512\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattn_mask_g\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    513\u001b[0m \u001b[43m        \u001b[49m\u001b[43mr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    514\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseg_mat\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    515\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmems\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmems\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    516\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    517\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    518\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    519\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    520\u001b[0m     output_h, output_g \u001b[38;5;241m=\u001b[39m outputs[:\u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m    522\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m output_g \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\myenv\\lib\\site-packages\\transformers\\models\\xlnet\\modeling_xlnet.py:439\u001b[0m, in \u001b[0;36mXLNetRelativeAttention.forward\u001b[1;34m(self, h, g, attn_mask_h, attn_mask_g, r, seg_mat, mems, target_mapping, head_mask, output_attentions)\u001b[0m\n\u001b[0;32m    436\u001b[0m k_head_r \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39meinsum(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mibh,hnd->ibnd\u001b[39m\u001b[38;5;124m\"\u001b[39m, r\u001b[38;5;241m.\u001b[39mtype(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mr\u001b[38;5;241m.\u001b[39mdtype), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mr)\n\u001b[0;32m    438\u001b[0m \u001b[38;5;66;03m# core attention ops\u001b[39;00m\n\u001b[1;32m--> 439\u001b[0m attn_vec \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrel_attn_core\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    440\u001b[0m \u001b[43m    \u001b[49m\u001b[43mq_head_h\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    441\u001b[0m \u001b[43m    \u001b[49m\u001b[43mk_head_h\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    442\u001b[0m \u001b[43m    \u001b[49m\u001b[43mv_head_h\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    443\u001b[0m \u001b[43m    \u001b[49m\u001b[43mk_head_r\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    444\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseg_mat\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseg_mat\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    445\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask_h\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    446\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    447\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    448\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    450\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n\u001b[0;32m    451\u001b[0m     attn_vec, attn_prob \u001b[38;5;241m=\u001b[39m attn_vec\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\myenv\\lib\\site-packages\\transformers\\models\\xlnet\\modeling_xlnet.py:281\u001b[0m, in \u001b[0;36mXLNetRelativeAttention.rel_attn_core\u001b[1;34m(self, q_head, k_head_h, v_head_h, k_head_r, seg_mat, attn_mask, head_mask, output_attentions)\u001b[0m\n\u001b[0;32m    279\u001b[0m \u001b[38;5;66;03m# position based attention score\u001b[39;00m\n\u001b[0;32m    280\u001b[0m bd \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39meinsum(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mibnd,jbnd->bnij\u001b[39m\u001b[38;5;124m\"\u001b[39m, q_head \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mr_r_bias, k_head_r)\n\u001b[1;32m--> 281\u001b[0m bd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrel_shift_bnij\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mklen\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mac\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    283\u001b[0m \u001b[38;5;66;03m# segment based attention score\u001b[39;00m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m seg_mat \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\myenv\\lib\\site-packages\\transformers\\models\\xlnet\\modeling_xlnet.py:258\u001b[0m, in \u001b[0;36mXLNetRelativeAttention.rel_shift_bnij\u001b[1;34m(x, klen)\u001b[0m\n\u001b[0;32m    254\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mreshape(x_size[\u001b[38;5;241m0\u001b[39m], x_size[\u001b[38;5;241m1\u001b[39m], x_size[\u001b[38;5;241m2\u001b[39m], x_size[\u001b[38;5;241m3\u001b[39m] \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    255\u001b[0m \u001b[38;5;66;03m# Note: the tensor-slice form was faster in my testing than torch.index_select\u001b[39;00m\n\u001b[0;32m    256\u001b[0m \u001b[38;5;66;03m#       However, tracing doesn't like the nature of the slice, and if klen changes\u001b[39;00m\n\u001b[0;32m    257\u001b[0m \u001b[38;5;66;03m#       during the run then it'll fail, whereas index_select will be fine.\u001b[39;00m\n\u001b[1;32m--> 258\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex_select\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marange\u001b[49m\u001b[43m(\u001b[49m\u001b[43mklen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlong\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    259\u001b[0m \u001b[38;5;66;03m# x = x[:, :, :, :klen]\u001b[39;00m\n\u001b[0;32m    261\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Number of training epochs (authors recommend between 2 and 4)\n",
    "epochs = 4\n",
    "\n",
    "# trange is a tqdm wrapper around the normal python range\n",
    "for i in range(epochs):\n",
    "  # Set our model to training mode (as opposed to evaluation mode)\n",
    "  model.train()\n",
    "\n",
    "  # Initialise epoch loss\n",
    "  epoch_loss = 0\n",
    "  \n",
    "  # Train the data for one epoch\n",
    "  for batch_number, batch in enumerate(train_dataloader):\n",
    "    # Unpack the inputs from our dataloader\n",
    "    batch_input_ids, batch_input_mask, batch_labels = batch\n",
    "      \n",
    "    # Clear out the gradients (by default they accumulate)\n",
    "    # Call optimizer.zero_grad() to reset the gradients of model parameters. Gradients by default add up; to prevent double-counting, we explicitly zero them at each iteration\n",
    "    optimizer.zero_grad()\n",
    "      \n",
    "    # Forward pass\n",
    "    # See https://huggingface.co/docs/transformers/v4.39.1/en/model_doc/xlnet#transformers.XLNetForSequenceClassification\n",
    "    # See https://huggingface.co/docs/transformers/en/model_doc/xlnet#transformers.models.xlnet.modeling_xlnet.XLNetForSequenceClassificationOutput\n",
    "    outputs = model(batch_input_ids, token_type_ids=None, attention_mask=batch_input_mask, labels=batch_labels)\n",
    "    loss = outputs[0]  \n",
    "\n",
    "    epoch_loss += loss.item()\n",
    "      \n",
    "    # Backward pass\n",
    "    # Compute derivatives of loss function wrt parameters\n",
    "    # When doing backward propagation, PyTorch accumulates the gradients, i.e. the value of computed gradients is added to the grad property of all leaf nodes of computational graph\n",
    "    loss.backward()\n",
    "      \n",
    "    # Update parameters and take a step using the computed gradient\n",
    "    # Adjust the parameters by the gradients collected in the backward pass\n",
    "    optimizer.step()\n",
    "    \n",
    "\n",
    "  print(\"Loss for epoch \" + str(epoch_number+1) + \" is: \" + str(epoch_loss / len(dataloader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e8f015-11ca-4168-a2d7-605fd5529ccb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
